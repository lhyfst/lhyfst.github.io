
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SphereHead</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!--     <meta property="og:image" content="https://jonbarron.info/zipnerf/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://lhyfst.github.io/dsfnet/"/>
    <meta property="og:title" content="DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment" />
    <meta property="og:description" content="Sensitivity to severe occlusion and large view angles limits the usage scenarios of the existing monocular 3D dense face alignment methods. The state-of-the-art 3DMM-based method, directly regresses the model's coefficients, underutilizing the low-level 2D spatial and semantic information, which can actually offer cues for face shape and orientation. In this work, we demonstrate how modeling 3D facial geometry in image and model space jointly can solve the occlusion and view angle problems. Instead of predicting the whole face directly, we regress image space features in the visible facial region by dense prediction first. Subsequently, we predict our model's coefficients based on the regressed feature of the visible regions, leveraging the prior knowledge of whole face geometry from the morphable models to complete the invisible regions. We further propose a fusion network that combines the advantages of both the image and model space predictions to achieve high robustness and accuracy in unconstrained scenarios. Thanks to the proposed fusion module, our method is robust not only to occlusion and large pitch and roll view angles, which is the benefit of our image space approach, but also to noise and large yaw angles, which is the benefit of our model space method. Comprehensive evaluations demonstrate the superior performance of our method compared with the state-of-the-art methods. On the 3D dense face alignment task, we achieve 3.80% NME on the AFLW2000-3D dataset, which outperforms the state-of-the-art method by 5.5%." /> -->

<!--     <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Sensitivity to severe occlusion and large view angles limits the usage scenarios of the existing monocular 3D dense face alignment methods. The state-of-the-art 3DMM-based method, directly regresses the model's coefficients, underutilizing the low-level 2D spatial and semantic information, which can actually offer cues for face shape and orientation. In this work, we demonstrate how modeling 3D facial geometry in image and model space jointly can solve the occlusion and view angle problems. Instead of predicting the whole face directly, we regress image space features in the visible facial region by dense prediction first. Subsequently, we predict our model's coefficients based on the regressed feature of the visible regions, leveraging the prior knowledge of whole face geometry from the morphable models to complete the invisible regions. We further propose a fusion network that combines the advantages of both the image and model space predictions to achieve high robustness and accuracy in unconstrained scenarios. Thanks to the proposed fusion module, our method is robust not only to occlusion and large pitch and roll view angles, which is the benefit of our image space approach, but also to noise and large yaw angles, which is the benefit of our model space method. Comprehensive evaluations demonstrate the superior performance of our method compared with the state-of-the-art methods. On the 3D dense face alignment task, we achieve 3.80% NME on the AFLW2000-3D dataset, which outperforms the state-of-the-art method by 5.5%."" /> -->


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ˜œ</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>SphereHead</b>: Stable 3D Full-head Synthesis with </br> Spherical Tri-plane Representation</br> 
                <small>
                <!-- CVPR 2023 -->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://lhyfst.github.io/">Heyuan Li<sup>1</sup></a>
                    </li>
                    <li>
                        Ce Chen<sup>1</sup>
                    </li>
                    <li>
                        Tianhao Shi<sup>1</sup>
                    </li>
                    <li>
                        Yuda Qiu<sup>1</sup>
                    </li>
                    <li>
                        <a href="https://sizhean.github.io/">Sizhe An<sup>2</sup></a>
                    </li>
                    <li>
                        <a href="https://guanyingc.github.io/">Guanying Chen<sup>1</sup></a>
                    </li>
                    <li>
                        <a href="https://gaplab.cuhk.edu.cn/pages/people">Xiaoguang Han<sup>1</sup></a>
                    </li>
                </ul>
                <p>
                    <sup>1</sup>CUHK-Shenzhen &ensp;
                    <sup>2</sup>Meta
                </p>
            </div>
        </div>







        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="img/spherehead_paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/spherehead_paper_image.png" height="60px">
                                <h4><strong>Supplementary</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/github_icon.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/data_icon.png" height="60px">
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <image src="img/vis.png" width="100%"> -->
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/SuppVideo_compressed_Trim.mp4" type="video/mp4" />
                </video>
						</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    While recent advances in 3D-aware Generative Adversarial Networks (GANs) have aided the development of near-frontal view human face synthesis, the challenge of comprehensively synthesizing a full 3D head viewable from all angles still persists. Although PanoHead proves the possibilities of using a large-scale dataset with images of both frontal and back views for full-head synthesis, it often causes artifacts for back views. Based on our in-depth analysis, we found the reasons are mainly twofold. 
                    First, from network architecture perspective, we found each plane in the utilized tri-plane/tri-grid representation space tends to confuse the features from both sides, causing "mirroring" artifacts (e.g., the glasses appear in the back).
                    Second, from data supervision aspect, we found that existing discriminator training in 3D GANs mainly focuses on the quality of the rendered image itself, and does not care much about its plausibility with the perspective from which it was rendered. This makes it possible to generate "face" in non-frontal views, due to its easiness to fool the discriminator.
                    In response, we propose SphereHead, a novel tri-plane representation in the spherical coordinate system that fits the human head's geometric characteristics and efficiently mitigates many of the generated artifacts. We further introduce a view-image consistency loss for the discriminator to emphasize the correspondence of the camera parameters and the images. 
                    The combination of these efforts results in visually superior outcomes with significantly fewer artifacts. 
                </p>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/tNcI-1Y9FW8" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
<br> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Artifacts Addressed
                </h3>
                <image src="img/fig_intro.png" width="100%">

                <p class="text-justify">
                    Two main types of face artifacts addressed in this work. All cases are sampled from PanoHead's latent space. (a-b) We name the first type as mirroring-face artifacts, due to the back face mirroring the identity, expression and accessories of the front face precisely.
                    (c-d) We name the second type as multiple-face artifacts, because in this scenario there might be more than one fake faces and their identities, expression and accessories are different from the front face. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dual Spherical Tri-plane Representation
                </h3>
                <center><image src="img/method_dual.png" width="100%"></center>
                <p class="text-justify">
                    (a) Tri-plane representation. (b) Spherical tri-plane representation. Reconstructed head geometry from single (c) sphere A and (d) sphere B, each showing (i) seam artifacts and (ii) polar artifacts. (e) The combination of two spheres in dual spherical tri-plane representation, (i) the seam of sphere A, (ii) the seam of sphere B. (f) Fusion weight map. (g-h) For each sphere, the weight approaches zero as the locations near the seam and poles.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Framework
                </h3>
                <image src="img/pipeline.png" width="100%">

                <p class="text-justify">
                    The framework of our proposed SphereHead. Given a sampled code $z$ and camera parameter $c$, SphereHead synthesizes a spherical tri-plane features $f_F$ by fusing two sub-feature groups $f_A$ and $f_B$. By volumetric rendering with the sampled features in $f_F$, SphereHead generates high-quality view-consistent full head images $I^{+}$. The discriminator learns to focus on the alignment between images and their viewpoints instructed by our view-image consistency loss, by introducing an additional negative data pairs consisting the real images and mismatched labels $c_s$.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Baseline Comparison
                </h3>
                <image src="img/fig_baseline_1.png" width="100%">

                <p class="text-justify">
                    Qualitative comparison with state-of-the-art methods. (a) GIRAFFHD, (b) StyleSDF, (c) EG3D fail to capture the complete head geometry and appearance. (d-f) PanoHead shows complete head generation, but the results suffer from mirroring artifacts ((d) left-right identical mirroring artifacts and (f) mirroring-face artifacts) and (e) multiple-face artifacts). (g-l) Ours SphereHead synthesizes full-head images of high visual quality and is free of artifacts exhibited by other methods.
                </p>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparison of Single-view Image Inversion
                </h3>
                <center><image src="img/supp_pti1.png" width="100%"></center>

                    <p class="text-justify">
                        Left column (1-16): results of PanoHead. Right column (17-32): results of our SphereHead.
                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Random Sampling Comparison
                </h3>
                <center><image src="img/supp_sample1.png" width="100%"></center>

                    <p class="text-justify">
                        The monocular images on the left are target images from FFHQ dataset. The upper and lower rows on the right portray are the results from PanoHead and our SphereHead model, respectively.
                    </p>

            </div>
        </div> -->



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>

</textarea>
                </div>
            </div>
        </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

    </div>
</body>
</html>
